TABLE OF CONTENTS
1. Quick kubectl basics
2. Cluster creation (kubeadm on EC2, kind)
3. K8s architecture (ASCII diagram + components)
4. Workloads: Pod, Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, CronJob //Load balancing
5. Pod status & RestartPolicy
6. Persistent Volumes & PersistentVolumeClaims (PV/PVC)
7. Services & Ingress (with example Ingress YAML)
8. Probes (liveness/readiness/startup)
9. Taints & Tolerations
10. Autoscaling: HPA, VPA, KEDA (quick commands + examples)
11. RBAC (Role, ClusterRole, RoleBinding, ClusterRoleBinding)
12. Useful kubectl commands & troubleshooting
13. Extras: kubeconfig, kubeadm join, upgrading, logs, port-forward
14. Useful YAML examples (Deployment, StatefulSet, DaemonSet, CronJob, PV/PVC, Ingress, HPA, Role/Binding)

---

1) QUICK kubectl BASICS
-----------------------
Configure kubectl:
  # copy kubeconfig to ~/.kube/config or set KUBECONFIG env var
  export KUBECONFIG=/etc/kubernetes/admin.conf

Basic commands:
  kubectl version --short
  kubectl cluster-info
  kubectl get nodes
  kubectl get pods --all-namespaces
  kubectl get svc -A
  kubectl get deployments -n myns
  kubectl describe pod <pod-name> [-n namespace]
  kubectl logs <pod> [-c container] [-n namespace]
  kubectl exec -it <pod> -- /bin/sh
  kubectl apply -f file.yaml
  kubectl delete -f file.yaml
  kubectl rollout status deployment/<deploy> -n <ns>
  kubectl rollout history deployment/<deploy> -n <ns>
  kubectl rollout undo deployment/<deploy> --to-revision=<rev>

2) CLUSTER CREATION
--------------------
A) kubeadm on EC2 (high level steps)
  - EC2: choose Ubuntu 22.04 or Amazon Linux 2. Use t3.medium or larger for control plane.
  - Open security group ports (control plane):
      TCP: 6443 (kube-apiserver)
      TCP: 2379-2380 (etcd)
      TCP: 10250 (kubelet), 10251 (kube-scheduler), 10252 (kube-controller-manager)
      UDP: 4789 (flannel VXLAN) or CNI-specific ports
      ICMP and SSH (22)
  - Swap must be disabled on each node: sudo swapoff -a (and remove from /etc/fstab)
  - Install container runtime (containerd) OR Docker (dockerd) + configure cgroup driver matching kubelet
  - Install kubeadm, kubelet, kubectl (same version)
    sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl
    curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
    sudo apt-get update
    sudo apt-get install -y kubelet kubeadm kubectl
    sudo apt-mark hold kubelet kubeadm kubectl
  - Initialize control plane (on master):
    sudo kubeadm init --pod-network-cidr=10.244.0.0/16
    # save the kubeadm join command printed at the end
  - Set up kubeconfig for ubuntu user:
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
  - Install CNI (example Flannel):
    kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
  - Join worker nodes:
    # run the saved kubeadm join <master-ip>:6443 --token ... --discovery-token-ca-cert-hash sha256:...
  - Verify nodes ready: kubectl get nodes

B) kind (Kubernetes in Docker) - great for local dev/testing
  - Install kind and docker
  - Create a simple cluster:
    cat <<EOF > kind-config.yaml
    kind: Cluster
    apiVersion: kind.x-k8s.io/v1alpha4
    nodes:
      - role: control-plane
      - role: worker
    networking:
      disableDefaultCNI: true
    EOF

    kind create cluster --config kind-config.yaml --name mykind
  - Install a CNI (weave, calico) or use built-in kind networking.

3) K8s ARCHITECTURE 
---------------------------
   Users / CI              kubectl
       |                     |
       v                     v
   +------------------------------------------------+
   |                Control Plane                   |
   |  +---------+   +---------------+   +---------+  |
   |  | etcd    |<->| kube-apiserver|<->| controllers| |
   |  +---------+   +---------------+   +---------+  |
   |        ^             ^             ^            |
   +--------|-------------|-------------|------------+
            |             |             |
            v             v             v
   +-----------------------------------------------+
   |                 Worker Nodes                  |
   |  +-----------+  +-----------+  +------------+  |
   |  | kubelet   |  | kube-proxy|  | containerd |  |
   |  +-----------+  +-----------+  +------------+  |
   |     Pod(s)        Services/iptables         PV |
   +-----------------------------------------------+

4) WORKLOADS & RESOURCE TYPES 
-------------------------------------
- Pod: smallest deployable unit (one or more containers, shared IP/namespace)
- Deployment: declarative updates for Pods/ReplicaSets (stateless apps)
- ReplicaSet: ensures N replicas of a Pod template (usually managed by Deployments)
- StatefulSet: stable network IDs, stable persistent storage (ordered, for DBs)
- DaemonSet: ensure one Pod per (selected) node (for log/monitor agents)
- Job: run-to-completion tasks (batch)
- CronJob: scheduled Jobs
- Service: stable network endpoint (ClusterIP, NodePort, LoadBalancer, ExternalName)
- Ingress: HTTP routing rules for external access (needs Ingress Controller)
- ConfigMap / Secret: configuration and sensitive data
- PersistentVolume (PV) / PersistentVolumeClaim (PVC): storage resources

5) POD STATUS & RESTART POLICY
------------------------------
View pod status:
  kubectl get pods -A
  kubectl get pods -o wide
  kubectl describe pod <pod> -n <ns>

Pod phases: Pending, Running, Succeeded, Failed, Unknown
Container states: Waiting (imagePullBackOff, CrashLoopBackOff), Running, Terminated

RestartPolicy (set on Pod spec):
  restartPolicy: Always   # restart containers on failure (default for Deployment)
  restartPolicy: OnFailure # restart only on failure (common for Jobs)
  restartPolicy: Never     # don't restart (use for one-shot pods)

6) PERSISTENT VOLUME
-------------------------------
Static provisioning example (hostPath for testing only):
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data/pv-hostpath
  persistentVolumeReclaimPolicy: Retain

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

For cloud (AWS) use EBS dynamic provisioning via StorageClass (provisioner: ebs.csi.aws.com).

7) SERVICE & INGRESS (example Ingress for nginx)
------------------------------------------------
Service (ClusterIP):
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

Ingress (example, needs ingress controller like ingress-nginx):
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  annotations:
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: nginx-svc
                port:
                  number: 80

8) PROBES (liveness/readiness/startup)
--------------------------------------
Common fields in container spec:
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 10
  failureThreshold: 3

readinessProbe:   # controls Service endpoints
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5

startupProbe:     # for slow-starting apps
  httpGet:
    path: /healthz
    port: 8080
  failureThreshold: 30
  periodSeconds: 10

9) TAINTS & TOLERATIONS
------------------------
Taint a node to repel pods:
  kubectl taint nodes <node-name> key=value:NoSchedule

Toleration in Pod spec (to allow scheduling on tainted nodes):
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"

10) AUTOSCALING: HPA, VPA, KEDA
-------------------------------
A) Horizontal Pod Autoscaler (HPA) - built-in
  # requires metrics-server or Prometheus adapter for custom metrics
  kubectl autoscale deployment myapp --min=1 --max=10 --cpu-percent=50
  kubectl get hpa
  kubectl describe hpa myapp

HPA YAML example:
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50

B) Vertical Pod Autoscaler (VPA) - adjusts resource requests/limits
  # install VPA controller from autoscaling/vpa project
  kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler.yaml
  # create VPA object
  apiVersion: autoscaling.k8s.io/v1
  kind: VerticalPodAutoscaler
  metadata: { name: myapp-vpa }
  spec:
    targetRef:
      apiVersion: "apps/v1"
      kind:       Deployment
      name:       myapp
    updatePolicy:
      updateMode: "Auto"   # or "Off" or "Initial"

C) KEDA (Kubernetes Event-driven Autoscaling) - for event-driven scaling (eg. queue length)
  # install KEDA operator
  kubectl apply -f https://github.com/kedacore/keda/releases/latest/download/keda-2.*/keda-operator.yaml
  # Create ScaledObject referencing deployment and trigger (e.g., RabbitMQ, Azure queue, Prometheus)
  apiVersion: keda.sh/v1alpha1
  kind: ScaledObject
  metadata:
    name: my-scaledobject
  spec:
    scaleTargetRef:
      name: my-deployment
    triggers:
      - type: prometheus
        metadata:
          serverAddress: http://prometheus:9090
          metricName: queue_length
          threshold: '100'

11) RBAC - Role Based Access Control 
--------------------------------------------
# View current RBAC
kubectl get clusterrole,clusterrolebinding,role,rolebinding -A

Example Role & RoleBinding (namespace-scoped):
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get","watch","list"]

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: dev
subjects:
  - kind: User
    name: "alice@example.com"
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

ClusterRole & ClusterRoleBinding are cluster-wide equivalents.

12) USEFUL kubectl & TROUBLESHOOTING
------------------------------------
View events:
  kubectl get events -A --sort-by='.metadata.creationTimestamp'
Describe objects for details:
  kubectl describe pod <pod> -n ns
Check node conditions:
  kubectl describe node <node>
Check kubelet logs on node (systemd):
  sudo journalctl -u kubelet -f
Port-forward for local access:
  kubectl port-forward svc/my-svc 8080:80 -n myns
Copy files from pod:
  kubectl cp mypod:/app/log.txt ./log.txt -n myns
Exec into init container:
  kubectl exec -it <pod> -c <init-container> -- sh

13) EXTRA: kubeadm join, upgrade, kubeconfig
--------------------------------------------
kubeadm join command (example printed after kubeadm init):
  kubeadm join <control-plane-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>

Upgrade Control Plane with kubeadm:
  sudo apt-get update && sudo apt-get install -y kubeadm=<newversion>
  sudo kubeadm upgrade plan
  sudo kubeadm upgrade apply <newversion>

14) YAML EXAMPLES 
------------------------------------

A) Deployment (nginx) with probes and PVC mount
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 20
        volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: html-volume
      volumes:
      - name: html-volume
        persistentVolumeClaim:
          claimName: pvc-claim

B) StatefulSet (postgres) (short)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: pg
spec:
  serviceName: "pg-svc"
  replicas: 3
  selector:
    matchLabels:
      app: pg
  template:
    metadata:
      labels:
        app: pg
    spec:
      containers:
      - name: postgres
        image: postgres:15
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: pgdata
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: pgdata
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi

C) DaemonSet (fluentd/log collector)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd:latest
        resources:
          limits:
            memory: "200Mi"
            cpu: "100m"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      volumes:
      - name: varlog
        hostPath:
          path: /var/log

D) CronJob (runs every hour)
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hourly-job
spec:
  schedule: "0 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: worker
            image: busybox
            args: ["sh", "-c", "echo hello; sleep 30"]

E) PersistentVolume + PVC (hostPath example)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data/pv-hostpath
  persistentVolumeReclaimPolicy: Retain

--- PVC ---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

F) HPA (v2 example with CPU)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50

G) RBAC - Role & RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get","watch","list"]

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: dev
subjects:
  - kind: User
    name: "alice@example.com"
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

---

15) QUICK REFERENCE - Commands (summary)
----------------------------------------
kubectl get nodes
kubectl get pods -A -o wide
kubectl get svc -A
kubectl get deployments -n <ns>
kubectl describe pod <pod> -n <ns>
kubectl logs -f <pod> -n <ns>
kubectl exec -it <pod> -- /bin/sh -n <ns>
kubectl apply -f file.yaml
kubectl delete -f file.yaml
kubectl port-forward svc/my-svc 8080:80 -n myns
kubectl top nodes
kubectl top pods
kubectl taint nodes node1 key=value:NoSchedule
kubectl autoscale deployment myapp --min=1 --max=5 --cpu-percent=60
kubectl get hpa
kubectl get pvc,pv
kubectl get ingress -A
